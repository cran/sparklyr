% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spark_compile.R
\name{spark_compile}
\alias{spark_compile}
\title{Compile Scala sources into a Java Archive}
\usage{
spark_compile(jar_name, spark_home = NULL, filter = NULL, scalac = NULL,
  jar = NULL, jar_dep = NULL)
}
\arguments{
\item{spark_home}{The path to the Spark sources to be used
alongside compilation.}

\item{filter}{An optional function, used to filter out discovered \code{scala}
files during compilation. This can be used to ensure that e.g. certain files
are only compiled with certain versions of Spark, and so on.}

\item{scalac}{The path to the \code{scalac} program to be used, for
compilation of \code{scala} files.}

\item{jar}{The path to the \code{jar} program to be used, for
generating of the resulting \code{jar}.}

\item{jar_dep}{An optional list of additional \code{jar} dependencies.}

\item{name}{The name to assign to the target \code{jar}.}
}
\description{
Given a set of \code{scala} source files, compile them
into a Java Archive (\code{jar}).
}
\keyword{internal}
